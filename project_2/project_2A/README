NAME: Joseph Picchi
EMAIL: jpicchi22@g.ucla.edu

DESCRIPTIONS OF EACH FILE INCLUDED IN "lab2a.tar.gz":

--> lab2_add.c
        A C source module designed to compare the viability, effectiveness, and efficiency of
        different locking mechanisms in the context of incrementing a variable shared among
        multiple threads. The resulting executable creates a user-specified number of threads,
        each of which increments, then decrements a shared variable for a user-specified number
        of iterations. The program also features a "--yield" flag that exacerbates the race
        condition, as well as a "--sync" option that allows the user to specify a locking mechanism
        (ie spin lock, mutex lock, or test-and-set) to protect the shared variable.

--> SortedList.h
        A header file supplied on CCLE that specifies the interface for doubly linked list operations.

--> SortedList.c
        A C source module that implements the interface described in "SortedList.h", namely
        methods to insert, delete, look up, and count items in a doubly linked list. This module
        also contains "yield" calls that exacerbate existing race conditions.

--> lab2_list.c
        A C source module that supplies a variety of command line options to test race conditions
        associated with the linked list operations described in "LinkedList.h". Namely,
        the resulting program offers a "--threads" flag to specify the number of threads that
        perform concurrent operations on the list, a "--iterations" flag to specify the number
        of iterations over which those operations are performed, a "--yield" flag to trigger the
        yield system calls in "SortedList.c", and a "--sync" flag to test the effectiveness of
        various locking mechanisms in solving linked list race conditions.

--> Makefile
        A makefile containing the following targets:
        --> build:
                The default target that compiles "lab2_list.c" and "lab2_add.c" into the executables
                "lab2_list" and "lab2_add" using the "-Wall" and "-Wextra" options.
        --> tests:
                Runs the test cases specified in the project spec to populate the "lab2_add.csv"
                and "lab2_list.csv" files.
        --> graphs:
                Uses the files "lab2_add.gp" and "lab2_list.gp" to generate the graphs described in
                the project spec via gnuplot.
        --> dist:
                Compresses all of the files described in this readme into a tarball named
                "lab2a-605124511.tar.gz".
        --> clean: 
                Deletes the "lab2_list" and "lab2_add" executables, as well as the tarball
                "lab2a-605124511.tar.gz".

--> lab2_add.csv
        Contains the results of the test cases assocated with "lab2_add", as specified in the
        project spec. This is the csv file that contains the data required to build the
        graphs for part 1 of the project.

--> lab2_list.csv
        Contains the results of the test cases assocated with "lab2_list", as specified in the
        project spec. This is the csv file that contains the data required to build the
        graphs for part 2 of the project.

--> lab2_add-1.png
        A graph generated by "lab2_add.gp" and "lab2_add.csv" that depicts the number of threads
        and iterations required to generate a failure (ie a nonzero final counter variable) when
        the program is run both with and without yields.

--> lab2_add-2.png
        A graph generated by "lab2_add.gp" and "lab2_add.csv" that depicts the average time
        to perform each operation, both with and without yields.

--> lab2_add-3.png
        A graph generated by "lab2_add.gp" and "lab2_add.csv" that depicts the average time
        to perform a single-threaded operation as a function of the number of iterations.

--> lab2_add-4.png
        A graph generated by "lab2_add.gp" and "lab2_add.csv" that depicts the number of
        threads and iterations that can run successfully (ie produce a zero-valued final
        counter variable) with yields under each synchronization option.

--> lab2_add-5.png
        A graph generated by "lab2_add.gp" and "lab2_add.csv" that depicts the average time
        to perform each protected operation as a function of the number of threads passed.

--> lab2_list-1.png
        A graph generated by "lab2_list.gp" and "lab2_list.csv" that depicts the average
        time to perform each single threaded unprotected operation as a function of the
        number of iterations passed.

--> lab2_list-2.png
        A graph generated by "lab2_add.gp" and "lab2_add.csv" that depicts the number
        of threads and iterations required to generate a failure (ie a corrupted list
        or a nonempty final list), both with and without yields.

--> lab2_list-3.png
        A graph generated by "lab2_add.gp" and "lab2_add.csv" that depicts the number
        of iterations that can run without failure for trials that are protected by
        synchronization options.

--> lab2_list-4.png
        A graph generated by "lab2_add.gp" and "lab2_add.csv" that depicts the cost
        to perform each operation as a function of the number of threads passed, where
        each trial is run with a synchronization technique and the results are corrected
        for list length.

--> test_cases.sh
        An extraneous file that is run by the Makefile's "tests" target. This file
        contains all of the test cases specified in the project spec that are used
        to generate the data for the afforementioned graphs.

--> lab2_add.gp
        A gnuplot script that generates graphs for the results of the "lab2_add"
        program.

--> lab2_list.gp
        A gnuplot script that generates graphs for the results of the "lab2_list"
        program.


RESEARCH SOURCES:
--> man pages for:
        signal
        sched_yield
        perror
        clock_gettime
        pthread_create
        pthread_join
--> all discussion slides and pseudocode from the "week5-section1B-slides" powerpoint presentation
    posted on CCLE.
--> the pthreads tutorial specified in the project spec that is located at this link:
    https://computing.llnl.gov/tutorials/pthreads
--> the GCC atomic builtins manual that was provided in the project spec at this link:
    http://gcc.gnu.org/onlinedocs/gcc-4.4.3/gcc/Atomic-Builtins.html


ANSWERS TO PROJECT QUESTIONS:

--> QUESTION 2.1.1 - causing conflicts:
        Why does it take many iterations before errors are seen? Why does a significantly smaller
        number of iterations so seldom fail?
            --> For a small number of iterations, most threads are able to complete all of their work
                (ie complete all iterations of the "add" function) before their time slice ends.
                Therefore, it is unlikely that any threads will yield in the middle of the "add"
                function and cause errors. In contrast, a larger number of iterations means that each
                thread must perform a greater quantity of work, thus increasing the probability that
                a thread's time slice will end before completing all iterations of the "add" function.
                Therefore, threads are more likely to yield in the middle of the "add" function and
                cause errors.


--> QUESTION 2.1.2 - cost of yielding:
        Why are the --yield runs so much slower? Where is the additional time going?
            --> The "--yield" runs are much slower because the OS interrupts each thread during every
                iteration of the "add" function, repeatedly transferring control of the CPU back to 
                the OS to decide which thread or process should run next. The additional time is going
                into context switches that save the state of the yielding thread and load in the state
                of a new thread, which is an expensive ordeal when performed frequently.

        Is it possible to get valid per-operation timings if we are using the --yield option? If so,
        explain how. If not, explain why not.
           --> No, it is not possible to get valid per-operation timings because the "yield" call occurs
                in the middle of the "add" operation, and we have no way to reliably determine the
                amount of time it takes to perform context switches and other OS-related operations.


--> QUESTION 2.1.3 - measurement errors:
        Why does the average cost per operation drop with increasing iterations?
           --> The cost per operation drops as iterations increase because there exist costs that are 
                inependent of the "add" operation itself (eg creating a thread). More iterations means
                that these baseline costs are amortized across a greater number of operations, thus
                reducing the cost per operation.

        If the cost per iteration is a function of the number of iterations, how do we know
        how many iterations to run (or what the "correct" cost is)?
           --> "lab2_add-3.png" indicates that the cost per operation decreases exponentially as the 
                number of iterations increases. Thus, when the graph begins to level out (ie a given
                change in iterations cultivates an insignificant change in cost per iteration), we
                can select a point in this region to indicate the "correct" number of iterations to
                run and the "correct" cost per iteration.


--> QUESTION 2.1.4 - costs of serialization:
        Why do all of the options perform similarly for low numbers of threads?
           --> For low numbers of threads, there is low contention to get the lock or to
                access the critical section, so threads don't have to wait as long to
                perform their work. This phenomenon is related to the workload and independent
                of the synchronization option, so all synchronization options display similarly
                low costs per iteartions for low numbers of threads.

        Why do the three protected operations slow down as the number of threads rises?
           --> As the number of threads rises, there exists greater contention to get the lock
                or access the critical section, so threads are forced to wait for longer periods
                of time in order to perform their work. This increases the cost per operation
                (and thus slows down) all three protected options.


--> QUESTION 2.2.1 - scalability of Mutex
        Compare the variation in time per mutex-protected operation vs the number of threads in
        Part-1 (adds) and Part-2 (sorted lists). Comment on the general shapes of the curves, and
        explain why they have this shape.
           --> In part 1, the cost per mutex-protected operation increases as the number of threads
                increases (ie the curve in "lab2_add-5.png" has a positive slope) because increased
                thread counts caused increased contention for the lock or critical section, thus leading
                to high wait times that increased the cost per operation.
                In contrast, in part 2, the cost per mutex-protected operation increases only slightly
                (if at all), leading to a relatively flat, horizontal curve. This is because the linked 
                list critical section remains locked for a longer period of time, so wait times are
                relatively high regardless of how many threads are spawned.

        Comment on the relative rates of increase and differences in the shapes of the curves, and
        offer an explanation for these differences.
           --> The rate of increase (ie the slope) is much greater in part 1 because the "add" operation is
                relatively quick, so a greater thread count rapidly increases lock contention and waiting
                times, thereby driving up the cost per operation relatively sharply. In contrast, the rate
                of increase (ie slope) is extremely low (if not 0) for part 2 due to the reasoning
                explained in my response to the first portion of this question.


--> QUESTION 2.2.2 - scalability of spin locks
        Compare the variation in time per protected operation vs the number of threads for list
        operations protected by Mutex vs Spin locks. Comment on the general shapes of the curves, and
        explain why they have this shape.
        Comment on the relative rates of increase and differences in the shapes of the curves, and
        offer an explanation for these differences.
           --> Cost per protected operation increases very slightly (ie the slope is positive, but very
                close to 0) for mutex locks, while the cost per operation increases more rapidly (ie
                the slope is more positive) for spin locks as thread count increases. The cost is
                approximately the same for low thread counts because there are few threads waiting to
                enter the critical section at any given time, so there are minimal performance differences
                between a sleeping thread (in mutex) versus a spinning thread (in spin locks) for low thread
                count. However, as thread count increases, cost per operation increases much more rapidly
                for spin locks because there are more threads waiting at any given time. These many
                waiting threads consume CPU cycles as they wait, thus increasing execution times and
                driving up the cost per operation in the spin lock implementation. In contrast, the cost
                barely increases for mutex locks because sleeping threads do not consume CPU cycles as
                they wait, since they are only awoken when they recieve signals.